{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480066b8",
   "metadata": {},
   "source": [
    "<h1 align='center'>Welcome to the AMD AI Premier League (AAIPL)!</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea12fc",
   "metadata": {},
   "source": [
    "<!-- <img src=\"./assets/aaipl.png\"> -->\n",
    "<img src=\"./assets/AMDAAIPL.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4b3fe",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Task:\n",
    "Here you will be building:\n",
    "1.  A question agent or Q-agent (e.g., [question_model.py](./agents/question_model.py) & [question_agent.py](./agents/question_agent.py)) that will ask some $N$ puzzle-based questions based on some given [topics](./assets/topics.json). *Note your question agent should output questions in the format specified in [sample_question.json](./assets/sample_question.json)*.\n",
    "2.  Also, an answer agent or A-agent (e.g., [answer_model.py](./agents/answer_model.py) & [answer_agent.py](./agents/answer_agent.py)) that answer questions asked from question agent. Here too, the format of the output should follow as specified in [sample_answer.json](./assets/sample_answer.json) file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed3cfa0",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1.  How to initiate your work station?\n",
    "    -   Type `http://xxx.xxx.xxx.xxx:8080` link (in *Chrome*), where `xxx.xxx.xxx.xxx` is the 12 (or 11) digit IP shared with each team. Hiting this URL, will open up a Jupyter lab page. Enter the token as `amdhack` (same for all).\n",
    "    -   Upon landing into Jupyter Lab page, on left side (i.e., folders), you will see `AAIPL/`, `hf_models/`. All this will be inside `/jupyter-tutorial` directory.\n",
    "    -   Within `hf_models/` there are some base checkpoints which will be used for question and answer agent creation. For both, Q-agent and A-agent, we are using `Qwen3-4B` (with `enable_thinking=False` to avoid thinking tokens) as base model.\n",
    "    -   Note that inside `AAIPL/`, there is a `tutorial` folder which consists of python scripts and `.ipynb` file demonstrating how to do *SFT*, *GRPO*, and *Prompt-tuning*. You are encouraged to, of course, improve/edit upon this (BYOC). No need to stick with this strictly.\n",
    "2.  You got 24 hrs to bake this cake!\n",
    "3.  All the BEST!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd0427",
   "metadata": {},
   "source": [
    "## üìö Table of Contents:\n",
    "- üèÅ [Welcome to the AMD AI Premier League (AAIPL)!](#welcome-to-the-amd-ai-premier-league-aaipl)\n",
    "- üìù [Task](#task)\n",
    "- ‚öôÔ∏è [Instructions](#instructions)\n",
    "- üèè [Tournament Overview](#tournament-overview)\n",
    "- üìã [Guidelines](#guidelines)\n",
    "    - [Naming Conventions](#naming-conventions)\n",
    "    - [Format Overview](#format-overview)\n",
    "- üõ†Ô∏è [What you will submit?](#Ô∏èwhat-you-will-submit)\n",
    "- ‚ö†Ô∏è [RESTRICTIONS](#restrictions)\n",
    "    - [ALLOWED](#allowed)\n",
    "- üìÇ [Directory & Files overview](#directory--files-overview)\n",
    "- üöÄ [Env Setup](#env-setup)\n",
    "- üéÆ [Let the GAME begin!!!](#let-the-game-begin)\n",
    "    - ü§î [Q-Agent](#q-agent)\n",
    "        - ‚úÖ [Basic format-checks for questions from Q-agent](#basic-format-checks-for-questions-from-q-agent)\n",
    "    - ü§ñ [A-agent](#a-agent)\n",
    "        - ‚úÖ [Basic format-checks for answers from A-agent](#basic-format-checks-for-answers-from-a-agent)\n",
    "- üèÖ [Evaluation](#evaluation)\n",
    "    - üìä [Scoring Criteria](#scoring-criteria)\n",
    "- ‚è± [Time Limit](#time-limit)\n",
    "<!-- - üèÜ [LeaderBoard UI/UX](#leaderboard-uiux) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386cb37",
   "metadata": {},
   "source": [
    "## Tournament Overview\n",
    "<!-- üèè  -->\n",
    "1.  All matches in this tournament will be **1v1** knockout format where two teams, Team-A vs Team-B, will compete with their Q-agent (question agent) and A-agent (answer agent). For simplicity think Q-agent to be bowler and A-agent to be batsman.\n",
    "2.  Like a cricket match, this would also have two innings:\n",
    "\n",
    "    -   1st inning:\n",
    "        *   $N$ Question from the Q-agent (Team-A) and their corresponding $N$ answers from the A-agent (Team-B).\n",
    "        *   Q-agent score (Team-A): Say, $40$\n",
    "        *   A-agent score (Team-B): $60$\n",
    "\n",
    "    -   2nd inning:\n",
    "        *   $N$ Question from the Q-agent (Team-B) and their respective $N$ responses from the A-agent (Team-A).\n",
    "        *   Q-agent score (Team-B): Say, $70$\n",
    "        *   A-agent score (Team-A): $30$\n",
    "    -   Final Score:\n",
    "        *   Team-A score $=$ 1st inning Q-agent score $+$ 2nd inning A-agent score $= 40 + 30 = 70$\n",
    "        *   Team-B score $=$ 1st inning Q-agent score $+$ 2nd inning A-agent score $= 60 + 70 = 130$\n",
    "\n",
    "    -   Winner: **Team-B** with a score of $130$.\n",
    "    -   For more info on <b> how SCORING is done</b>, kindly refer to this [cell](#scoring-criteria).\n",
    "\n",
    "<u>NOTE</u>: In case of **TIE**, we will use some (closed) benchmark questions, we will evaluate your answer agents (A-agent) and rank the teams accordingly.\n",
    "\n",
    "**Whichever Team's Q-agent fails to generate atleast $50\\%$ of `num_questions` (where `num_questions` ranges from $2$ to $1000+$) of the questions correctly (as per [format-checking](#format-overview)) will be automatically disqualified.**<br>\n",
    "<u>Note</u>: Here $N$ denotes the number of filtered / format-correct questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deab9cf",
   "metadata": {},
   "source": [
    "\n",
    "## Guidelines:\n",
    "<!-- üìã  -->\n",
    "#### Naming Conventions:\n",
    "<ol type=\"a\">\n",
    "    <li>Rename this whole folder as <code>AAIPL_your_IP</code> if not done already. This <code>your_IP</code> will be <code>_</code> separated IPv4 address, no special-characters allowed. Follow the below <a href=\"#what-you-will-submit\">cell</a> for more info</li>\n",
    "    <li> For Q-agent:\n",
    "        <ol type=\"i\">\n",
    "            <li>For Q-agent wrapper <code>.py</code> file: <code>agents/question_agent.py</code>.</li>\n",
    "            <li>For Q-agent model <code>.py</code> file: <code>agents/question_model.py</code>.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li> For A-agent:\n",
    "        <ol type=\"i\">\n",
    "            <li>For A-agent wrapper <code>.py</code> file: <code>agents/answer_agent.py</code>.</li>\n",
    "            <li>For A-agent model <code>.py</code> file: <code>agents/answer_model.py</code>.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "#### Format Overview\n",
    "-   <u>Q-Agent</u>: Given a topic, the Q-agent should generate questions in the specified JSON format:\n",
    "    ```json\n",
    "    {\n",
    "    \"topic\": \"<Topic of the Question>\",\n",
    "    \"question\": \"<full question text>\",\n",
    "    \"choices\": [\n",
    "        \"A) <choice A text>\",\n",
    "        \"B) <choice B text>\",\n",
    "        \"C) <choice C text>\",\n",
    "        \"D) <choice D text>\"\n",
    "    ],\n",
    "    \"answer\": \"<correct choice letter only>\",\n",
    "    \"explanation\": \"brief explanation within 100 words for why the answer is correct\"\n",
    "    }\n",
    "    ```\n",
    "    from which we will extract **ONLY** the **\"Question\"** and **\"Choices\"** keys and feed it to the answer agent. The **\"Topic\"**, **\"Question\"**, **\"Choices\"**, and **\"Answer\"** will be verified for correctness from an Oracle.\n",
    "-   <u>A-agent</u>: Given a Question and Choices, A-agent should produce answer in the format of:\n",
    "    ```json\n",
    "    {\n",
    "        \"answer\": \"<correct choice letter only>\",\n",
    "        \"reasoning\": \"brief reasoning within 100 words for why the answer is correct\"\n",
    "    }\n",
    "    ```\n",
    "    where we will extract ONLY the **\"Answer\"** key and compare it with **\"Answer\"** from the opponent's question.\n",
    "-   *<u>Remarks</u>: Having explanation and reasoning is a plus. Not having them doesn't disqualify the question or answer being correct.*\n",
    "    \n",
    "**<u>Note</u>**: *We will only consider those responses from the Q-agent and the A-agent which follow the above format.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858a803",
   "metadata": {},
   "source": [
    "## What you will submit?\n",
    "<!-- üõ†Ô∏è  -->\n",
    "You need to submit your code which should contain these main files:\n",
    "\n",
    "<!-- *   `q_agent.py` (with one arg as `num_quetions: int`) - On running with `num_questions=20` should generate 20 questions in the required format as above.\n",
    "*   `a_agent.py` (with two args as `Question: str` and `Choices: List[str]`) - On running with the above two args should produce the o/p in the required format as above. -->\n",
    "<!-- 1. Submit the whole folder AAIPL with its name modified to `AAIP_<your_team_name_in_alphanumeric>`. No special characters, e.g., `#`, `@`, `!`, etc. are allowed in the team name. \n",
    "   - Example: `AAIP_Team1` or `AAIP_Team23` or `AAIP_Win47` are valid, but `AAIP_Team#1` or `AAIP_Team@1` are not.\n",
    "2. Also put Checkpoints (e.g., `model.safetensors` or `.pt` or `.pth`) file (situated at e.g., `ckpts/`) - given that they get successfully loaded automatically, when we execute inference as done above for both, question and answer agent.\n",
    "3. `requirements.txt` - This file lists all the extra dependencies required to run your agents apart from `default_requirements.txt`. -->\n",
    "\n",
    "1. Rename the `AAIPL` folder to `AAIPL_<your_IP_address>` if not done already. NO special characters, e.g., `#`, `@`, `!`, etc. are allowed except underscore, `_`, in the team name. \n",
    "   - Example: `AAIPL_192_154_162_143` or `AAIPL_192_154_182_14` are valid, but `AAIPL_Team#1` or `AAIPL_Team@1` are not. \n",
    "1. **No need to upload anything to anywhere, we'll collect your codes at sharp 2:00 PM - 13th July, 2025 from your Jupyter Server.**\n",
    "2. <span style=\"color: red;\">Don't forget to add your PPT (in `solution.pdf` format) summarizing the techniques you adopted to execute this task better, relative to this file (i.e., just inside `AAIPL_xxx_xxx_xxx_xxx` folder).</span>\n",
    "3. **ENSURE model checkpoint(s) (e.g., `model.safetensors` or `.pt` or `.pth`) is(are) loading and expected files are getting generated from Q-agent and A-agent, when inference is done. And put all your checkpoints in the `ckpt/` folder, located just inside `AAIPL_<your_IP>/`.**\n",
    "4. **<u>NOTE</u>: You are not required to generate any `.json` for us, we'll do that for you during evaluation setting a specific value to $N$.**\n",
    "\n",
    "<u><span style=\"color: blue\">NOTE</span></u>: These files will be checked for any hardcoding, RAG, or other unfair practices.<br>\n",
    "<u><span style=\"color: red\">REMARKS / CAUTION</span></u>: A-agent is equally important as Q-agent. So, please do focus on both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1cc2ce",
   "metadata": {},
   "source": [
    "## RESTRICTIONS\n",
    "<!-- ‚ö†Ô∏è -->\n",
    "\n",
    "1.  Kindly don't use any sort of ***RAG (Retrieval Augmented Generation)*** techniques. If found, the submission won't be considered for further evaluations.\n",
    "2.  **Usage of base models other than what given for Question (i.e., `Qwen3-4B`) and Answer (i.e., again `Qwen3-4B`) agent, will lead to disqualification.**\n",
    "3.  Do follow the guidelines as mentioned in [What you will submit?](#what-you-will-submit) section.\n",
    "4.  **<span style=\"color: red\">NO</span> LAST Minute Submission**: The submission deadline is strict. Upload link will expires just one minute before the deadline. So, please make sure you submit your code well in advance.\n",
    "5.  Any **<span style=\"color: red\">HACKEY</span>** approach or **hard-coding** will lead to disqualification.\n",
    "    -   E.g., Any hard-coded *adversarial attacks* that make A-agent hallucinates.\n",
    "6.  **Language Restriction**: ONLY English language is allowed for both Q-agent and A-agent. Any other language will lead to disqualification.\n",
    "7.  Strictly stay within the `max_tokens` limit as specified in `agen.yaml` & `qgen.yaml`. While other parameters can be changed as per your convenience.\n",
    "8.  $N$ should be passed as an argument to `question_agent.py`. We'll test for $N=1$. `--num_questions` is the argument.\n",
    "9.  Ensure **$40\\%$** of the questions you generate gets filtered into `questions.json`.\n",
    "\n",
    "\n",
    "### ALLOWED\n",
    "<!-- ‚úÖ  -->\n",
    "1.  Participants are encouraged to modify the code scripts (for any sort of training, data construction, inference, such that the above constraints are not overruled).\n",
    "2.  If you want to add `WANDB_API_KEY` for `wandb` logging do it in add `WANDB_API_KEY=xxxxxxx` before `python -m <script>.py` command. E.g., `!WANDB_API_KEY=xxxxxxx python -m agents.question_agent \\`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f34ed1",
   "metadata": {},
   "source": [
    "## Token & Time Limit:\n",
    "<!-- ‚è±  -->\n",
    "*   Maximum length (e.g., `max_token`) limit for your model response should be within following tokens.\n",
    "    *   For question-agent (Q-Agent) it is $100$ tokens cumulatively for the content corresponding to [`topic`, `question`, `choices`, and `answer`]. This excludes token count for double quotes as well as string length for topic, question, choices, and answer string it\n",
    "    *   And the rest is for explanation i.e., $1024-100 = 924$. But within time limit\n",
    "*   `ckpt` is the folder üìÇ which you will place under `AAIPL_XXX_XXX_XXX_XXX`. While `checkpoints` folder üìÇ inside `tutorial/` is meant for tutorial.\n",
    "*   Each question should be generated under `10 secs`. Overall time for 100 questions should be no more than `1000 secs` ~ `17 mins`.\n",
    "*   Each answer should be generated under `6 secs`. Overall time for 100 answers should be no more than `600 secs` ~ `10 mins`.\n",
    "*   *Note: We will only consider those questions' and answers' JSON file that remain under the time limit.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b562cb4",
   "metadata": {},
   "source": [
    "### Directory & Files overview\n",
    "<!-- üìÇ  -->\n",
    "```\n",
    ".\n",
    "‚îú‚îÄ‚îÄ agents\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ question_model.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ question_agent.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ answer_model.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ answer_agent.py\n",
    "‚îú‚îÄ‚îÄ tutorial # guide on how to SFT and GRPO\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ checkpoints #\n",
    "‚îÇ   ‚îÇ     ‚îú‚îÄ‚îÄ sft # save sft checkpoints here while in tutorial\n",
    "‚îÇ   ‚îÇ     ‚îú‚îÄ‚îÄ grpo # save grpo checkpoints here while in tutorial\n",
    "‚îÇ   ‚îÇ     ‚îî‚îÄ‚îÄ demo\n",
    "‚îÇ   ‚îÇ         ‚îú‚îÄ‚îÄ sft # pre-trained sft (LoRA) ckpt\n",
    "‚îÇ   ‚îÇ         ‚îî‚îÄ‚îÄ grpo # same as above but for GRPO\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ tutorial.ipynb # guide on how to SFT and GRPO\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ trainer.py # sample training for Gemma with LoRA (SFT) and GRPO\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ answer_model2.py # inference script for the same. Copy it to agents/. for using this as question generator\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ formatted_questions_array.json # Sample question data for doing SFT and GRPO\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test_questions_array.json # Sample test question to evaluate the SFTed or GRPOed model\n",
    "‚îú‚îÄ‚îÄ assets\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ topics_example.json # example questions w.r.t each topic\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ topics.json # Topics on which we require to generate questions\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sample_question.json # File specifying expected format of questions generated\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sample_answer.json # Expected format of answers generated\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ AMDAAIPL.png # Teaser image for the AAIPL\n",
    "‚îú‚îÄ‚îÄ utils\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ build_prompt.py # prompt-tuning scripts\n",
    "‚îú‚îÄ‚îÄ README.ipynb\n",
    "‚îú‚îÄ‚îÄ outputs # That will consists of outputs from question_agent.py and answer_agent.py\n",
    "‚îú‚îÄ‚îÄ ckpt # That will consists of checkpoints for question_agent.py and answer_agent.py if any training is done.\n",
    "‚îú‚îÄ‚îÄ qgen.yaml # Generation specific parameters for Q-agent\n",
    "‚îú‚îÄ‚îÄ agen.yaml # Generation specific parameters for A-agent\n",
    "‚îî‚îÄ‚îÄ default_requirements.txt # Packages required\n",
    "```\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc970e",
   "metadata": {},
   "source": [
    "### Env Setup\n",
    "<!-- üöÄ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308a46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install the necessary packages\n",
    "!pip install -r default_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50090b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic packages\n",
    "import json\n",
    "from typing import Dict, Any, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187a198",
   "metadata": {},
   "source": [
    "### Let the GAME begin!!!\n",
    "<!-- üéÆ  -->\n",
    "#### Q-Agent\n",
    "<!-- ü§î -->\n",
    "<u>NOTE</u>: You are encouraged to invoke your own custom code into `question_model.py` and `question_agent.py` at `agents/`, to control its operation, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fdb0b0",
   "metadata": {},
   "source": [
    "__Topics:__\n",
    "1.  `Logical Reasoning`: Truth-teller and Liar Problems\n",
    "2.  `Puzzles`: Seating Arrangements (Linear, Circular)\n",
    "3.  `Blood Relations and Family Tree`: Puzzles involving generations and family tree logic\n",
    "\n",
    "*To know what all topics are available, visit: **[topics.json](assets/topics.json)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f181848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the following code to generate questions.\n",
    "# For demo purpose, we have used the base Qwen3-4B model for Q-Agent. Participants are expected to improve upon this\n",
    "!python -m agents.question_agent \\\n",
    "    --output_file \"outputs/questions.json\" \\\n",
    "    --num_questions 250 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e511c33",
   "metadata": {},
   "source": [
    "#### Basic format-checks for questions from Q-agent\n",
    "1. Here we filter questions into `questions.json` for usage of answer agent.\n",
    "2. Further, the filtered questions will pass through an **`Oracle`** (a part of JUDGING system, hence closed and not demonstrated here) that checks *validity* of question, choices, and answer from Q-agent. It also provides the actual correct answer to the question.\n",
    "3. BYOC (Bring Your Own Code): Well, again we emphasize to have your own innovations & code. Also the places with following tag/block or **similar**, expect some real improvements.\n",
    "    ```python\n",
    "    # TODO: IMPROVE THE FOLLOWING\n",
    "    <code>\n",
    "    ```\n",
    "4. <span style=\"color : red\">**Ensure**</span> on an average: $50\\% \\times \\text{num\\_questions} > N$ questions are filtered out.\n",
    "5. The following filter is added into the `question_agent.py`. *<span style=\"color : red\">Note that</span> we generate two version of questions, one is the usual, unfiltered one `questions.json` and the other is `filtered_questions.json` after passing through the below filter. <span style=\"color : green\">We'll use this `filtered_questions.json` for conducting matches i.e., this file will be sent to opponent's answer agent. But do keep both in `outputs/` folder.</span>*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3770ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/jupyter-tutorial/hf_models/Qwen3-4B\", padding_side='left')\n",
    "\n",
    "def count_tokens_q(text: str) -> int:\n",
    "    \"\"\"Count the number of tokens using Qwen3-4B tokenizer\"\"\"\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def filter_questions(questions: List[str|Dict[str, str|Any]]) -> List[Dict[str, str|Any]]:\n",
    "    def basic_checks(q2: Dict[str, str])->bool:\n",
    "        # check required keys\n",
    "        required_keys = ['topic', 'question', 'choices', 'answer']\n",
    "        if all((key in q2) for key in required_keys):\n",
    "            # check choices format\n",
    "            checks = all(isinstance(choice, str) and len(choice) > 2 and choice[0].upper() in 'ABCD' for choice in q2['choices'])\n",
    "            if isinstance(q2['choices'], list) and len(q2['choices']) == 4 and checks:\n",
    "                # check answer format\n",
    "                # Check token length\n",
    "                check_len = sum(count_tokens_q(q2[k]) for k in ['question', 'answer'])\n",
    "                check_len += sum(count_tokens_q(choice) for choice in q2['choices']) - 15\n",
    "                if check_len < 130:\n",
    "                    if check_len + count_tokens_q(q2.get('explanation', 'None')) <= 1024:\n",
    "                        # Extra Checks: (PLUS checks) len(q2['answer']) == 1 and q2['answer'].upper() in 'ABCD':\n",
    "                        if isinstance(q2['answer'], str):\n",
    "                            return True\n",
    "        return False\n",
    "    correct_format_question = []\n",
    "    for i, q in enumerate(questions):\n",
    "        if isinstance(q, dict):\n",
    "            if basic_checks(q):\n",
    "                correct_format_question.append(q)\n",
    "        elif isinstance(q, str):\n",
    "            try:\n",
    "                q1 = json.loads(q)\n",
    "                if basic_checks(q1):\n",
    "                    correct_format_question.append(q1)\n",
    "            except json.JSONDecodeError:\n",
    "                # If JSON decoding fails, skip this answer\n",
    "                print(f\"Skipping invalid JSON at index {i}: {q}\")\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "    if len(correct_format_question) >= 0.5 * len(questions):\n",
    "        return correct_format_question\n",
    "    return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66e521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"outputs/valid_questions_26.json\", \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "filtered_questions = filter_questions(questions)\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Further filtering will happen with our Oracle (not shown here) which also have its own answer for the question.\n",
    "# If Q-agent answer to its own question is wrong, then that question will not be considered.\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "with open(\"outputs/filtered_questions.json\", \"w\") as f:\n",
    "    json.dump(filtered_questions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fe904",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96209d",
   "metadata": {},
   "source": [
    "#### A-agent\n",
    "<!-- ü§ñ  -->\n",
    "<u>NOTE</u>: Here in the `answer_agent.py` you can integrate your custom model -- ***E.g., SFTed or GRPOed model [answer_model2.py](./tutorial/answer_model2.py)**. But first do SFT / GRPO -> load the checkpoint with correct path in [answer_model2.py](./tutorial/answer_model2.py) and then integrate it into `answer_agent.py`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a35d18-485b-494f-98ea-aa8c8576dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m agents.answer_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51af0a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Same instructions apply for the answer agent.\n",
    "# For demo purpose, we have used the base Qwen3-4B model for A-agent. Participants are expected to improve upon this.\n",
    "!python -m agents.answer_agent \\\n",
    "    --input_file \"outputs/filtered_questions.json\" \\\n",
    "    --output_file \"outputs/answers.json\" \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3891529b",
   "metadata": {},
   "source": [
    "#### Basic format-checks for answers from A-agent.\n",
    "\n",
    "1. Checks for expected `JSON` format as suggested in [instructions](#format-overview).\n",
    "2. Same as Q-Agents, improvements are required here too.\n",
    "3. Answers not having above format will not be considered and thus no point will be awarded.\n",
    "4. The following filter is added into the `answer_agent.py`. Similarly here too, two versions are saved, `answers.json` and `filtered_answers.json`. The latter is used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acad45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/jupyter-tutorial/hf_models/Qwen3-4B\", padding_side='left')\n",
    "\n",
    "def count_tokens_a(text: str) -> int:\n",
    "    \"\"\"Count the number of tokens in the text using the agent's tokenizer\"\"\"\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def filter_answers(ans: List[str|Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    r\"\"\"Filter answers to ensure they are in the correct format\"\"\"\n",
    "    def basic_checks(a1: Dict[str, str])->bool:\n",
    "        # check required keys\n",
    "        required_keys = ['answer']\n",
    "        if all((key in a1) and isinstance(a1[key], str) for key in required_keys):\n",
    "            if len(a1['answer']) == 1 and (a1['answer'] not in 'ABCDabcd'):\n",
    "                    return False\n",
    "            check_len = count_tokens_a(a1['answer'])\n",
    "            if check_len < 50:\n",
    "                check_len += count_tokens_a(a1.get('reasoning', 'None'))\n",
    "                if check_len < 512:\n",
    "                    # check answer format - EXTRA checks\n",
    "                    # if len(a1['answer']) == 1 and a1['answer'].upper() in 'ABCD':\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    filtered_answers = []\n",
    "    for i, a in enumerate(ans):\n",
    "        if isinstance(a, dict):\n",
    "            if basic_checks(a):\n",
    "                filtered_answers.append(a)\n",
    "            else:\n",
    "                filtered_answers.append(None)\n",
    "        elif isinstance(a, str):\n",
    "            # Basic checks: at least with correct JSON format\n",
    "            try:\n",
    "                a1 = json.loads(a)\n",
    "                if basic_checks(a1):\n",
    "                    filtered_answers.append(a1)\n",
    "                else:\n",
    "                    filtered_answers.append(None)\n",
    "            except json.JSONDecodeError:\n",
    "                # If JSON decoding fails, skip this answer\n",
    "                print(f\"Skipping invalid JSON at index {i}: {a}\")\n",
    "                filtered_answers.append(None)\n",
    "                continue\n",
    "        else:\n",
    "            # If the answer is neither a dict nor a str, skip it\n",
    "            print(f\"Skipping unsupported type at index {i}: {type(a)}\")\n",
    "            filtered_answers.append(None)\n",
    "    return filtered_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4301d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"outputs/answers.json\", \"r\") as f:\n",
    "    answers = json.load(f)\n",
    "filtered_answers = filter_answers(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74567658-996c-4cf0-bdc3-a50140126051",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a6a911",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "<!-- üèÖ  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa2284",
   "metadata": {},
   "source": [
    "##### Scoring Criteria\n",
    "\n",
    "<!-- üìä  -->\n",
    "\n",
    "Simply, we assign scores based on, out of $N$ questions from Q-agent, how many an A-agent can answer and vice-versa. *No negative marking for wrong answers.*\n",
    "\n",
    "$$\\text{A-agent Score} = \\dfrac{\\#\\ \\text{of questions correctly answered with expected format}}{N}\\times 100$$\n",
    "$$\\text{Q-agent Score} = \\dfrac{\\#\\ \\text{of questions incorrectly answered by A-agent}}{N}\\times 100$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec36efc-8612-4833-81b6-55b129111726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd6a4e-4657-4840-aedb-1df75d46e2fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in filtered_answers:\n",
    "    if a is not None: print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f7ccf",
   "metadata": {},
   "source": [
    "**An Example demonstrating how Q-agent matches up with A-agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c11592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate scores...\n",
    "N = len(filtered_questions)\n",
    "assert N == len(filtered_answers), \"Number of questions and answers must match.\"\n",
    "num_correct_answers = len([1 for q,a in zip(filtered_questions, filtered_answers) if a is not None and q['answer'] == a['answer']])\n",
    "\n",
    "# Here the answer may be correct, but since q['answer'] is not an option letter is not there, we face problems\n",
    "# Below shown is one way of simple string parsing\n",
    "num_correct_answers = len([1 for q,a in zip(filtered_questions, filtered_answers) if a is not None and q['answer'][0] == a['answer']])\n",
    "\n",
    "a_score = num_correct_answers*100/(N+1e-9)\n",
    "q_score = (N-num_correct_answers)*100/(N+1e-9)\n",
    "# Announce the scores\n",
    "print(f\"Number of questions: {N}\")\n",
    "print(f\"Number of correct answers: {num_correct_answers}\")\n",
    "print(\"Scores:\")\n",
    "print(f\"Team B: A-agent score: {a_score:.2f}\")\n",
    "print(f\"Team A: Q-agent score: {q_score:.2f}\")\n",
    "print(f\"Innings 1 winner: {'Team A' if q_score > a_score else 'Team B' if q_score < a_score else 'Draw'}\")\n",
    "# DRAW case is not HANDLED now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45378998-e104-485f-a643-ae8a1cf1e059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
